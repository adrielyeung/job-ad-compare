{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Search Page Scraper\n",
    "This project aims to find out the most popular skills required in a certain role.\n",
    "\n",
    "Proposed method is to use web scraping (BeautifulSoup + Selenium) to extract many job listings from popular job search websites, then find the most popular words (keywords) used in their description using Tf-Idf techniques as provided by Scikit-Learn's ```CountVectorizer``` and ```TfIdfTransformer```.\n",
    "\n",
    "## 1. Web scraping\n",
    "Details of experimentation can be found in ```Scraping_Test.ipynb```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import requests\n",
    "import pprint\n",
    "import re\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_ad_sites = pd.read_csv('Config\\job_ad_sites.csv')\n",
    "job_ad_sites.fillna(-999999, inplace=True)\n",
    "job_ad_sites.Result_item = job_ad_sites.Result_item.astype(int)\n",
    "job_ad_sites.Title_item = job_ad_sites.Title_item.astype(int)\n",
    "job_ad_sites.Company_item = job_ad_sites.Company_item.astype(int)\n",
    "job_ad_sites.Location_item = job_ad_sites.Location_item.astype(int)\n",
    "job_ad_sites.Description_item = job_ad_sites.Description_item.astype(int)\n",
    "job_ad_sites.URL_item = job_ad_sites.URL_item.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished successfully\n"
     ]
    }
   ],
   "source": [
    "current_datetime = datetime.now()\n",
    "normal_form = \"NFKC\"\n",
    "# Headless option meaning not show browser window\n",
    "options = Options()\n",
    "options.headless = True\n",
    "options.add_argument(\"--window-size=1920,1200\")\n",
    "\n",
    "DRIVER_PATH = '<set up Chrome web driver path>'\n",
    "# driver = webdriver.Chrome(executable_path=DRIVER_PATH)\n",
    "driver = webdriver.Chrome(options=options, executable_path=DRIVER_PATH)\n",
    "# Store stdout as object (restore later)\n",
    "stdout_obj = sys.stdout\n",
    "with open(\"JobReport_\" + current_datetime.strftime(\"%Y%m%d_%H%M%S\") + \".txt\", \"w\", encoding = 'utf-8') as report_file:\n",
    "    # Set everything printed to stdout to file instead\n",
    "    sys.stdout = report_file\n",
    "    count = 0\n",
    "    print(\"Job Report on \" + current_datetime.strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "    print(\"=================================\")\n",
    "    for index, row in job_ad_sites.iterrows():\n",
    "        page = requests.get(row['Listing_URL'])\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        if row['Result_attribute'] == \"id\":\n",
    "            results = soup.find_all(row['Result_name'], id=row['Result_tag'])[int(row['Result_item'])]\n",
    "        elif row['Result_attribute'] == 'class':\n",
    "            results = soup.find_all(row['Result_name'], class_=row['Result_tag'])[int(row['Result_item'])]\n",
    "        if row['Element_attribute'] == \"id\":\n",
    "            job_elems = results.find_all(row['Element_name'], id=row['Element_tag'])\n",
    "        elif row['Element_attribute'] == 'class':\n",
    "            job_elems = results.find_all(row['Element_name'], class_=row['Element_tag'])\n",
    "\n",
    "        for job_elem in job_elems:\n",
    "            title_elem = None\n",
    "            company_elem = None\n",
    "            location_elem = None\n",
    "            URL = None\n",
    "            description_elem = None\n",
    "            content = None\n",
    "            if row['Title_attribute'] == \"id\":\n",
    "                title_elem = job_elem.find_all(row['Title_name'], id=row['Title_tag'])[int(row['Title_item'])]\n",
    "            elif row['Title_attribute'] == 'class':\n",
    "                if len(job_elem.find_all(row['Title_name'], class_=row['Title_tag'])) > 0:\n",
    "                    title_elem = job_elem.find_all(row['Title_name'], class_=row['Title_tag'])[int(row['Title_item'])]\n",
    "            elif row['Title_attribute'] == 'none':\n",
    "                title_elem = job_elem.find_all(row['Title_name'])[row['Title_item']]\n",
    "            if row['Company_attribute'] == \"id\":\n",
    "                company_elem = job_elem.find_all(row['Company_name'], id=row['Company_tag'])[int(row['Company_item'])]\n",
    "            elif row['Company_attribute'] == 'class':\n",
    "                if len(job_elem.find_all(row['Company_name'], class_=row['Company_tag'])) > 0:\n",
    "                    company_elem = job_elem.find_all(row['Company_name'], class_=row['Company_tag'])[int(row['Company_item'])]\n",
    "            if row['Location_attribute'] == \"id\":\n",
    "                location_elem = job_elem.find_all(row['Location_name'], id=row['Location_tag'])[int(row['Location_item'])]\n",
    "            elif row['Location_attribute'] == 'class':\n",
    "                if len(job_elem.find_all(row['Location_name'], class_=row['Location_tag'])) > 0:\n",
    "                    location_elem = job_elem.find_all(row['Location_name'], class_=row['Location_tag'])[int(row['Location_item'])]\n",
    "            if row['URL_attribute'] == \"id\":\n",
    "                URL = job_elem.find('a', id=row['URL_tag'])\n",
    "            elif row['URL_attribute'] == 'class':\n",
    "                URL = job_elem.find('a', class_=row['URL_tag'])\n",
    "            elif row['URL_attribute'] == \"none\":\n",
    "                URL = job_elem.find('a')\n",
    "            elif row['URL_attribute'] == \"self\":\n",
    "                URL = job_elem\n",
    "\n",
    "            if not any((title_elem, company_elem, location_elem, URL)):\n",
    "                count += 1\n",
    "                continue\n",
    "\n",
    "            URL = URL['href']\n",
    "\n",
    "            if str.startswith(URL, '/'):\n",
    "                URL = row['Prefix_URL'] + URL\n",
    "\n",
    "            try:\n",
    "                page_content = driver.get(URL)\n",
    "                if row['Description_attribute'] == 'id':\n",
    "                    content = driver.find_element_by_id(row['Description_tag'])\n",
    "                elif row['Description_attribute'] == 'class':\n",
    "                    content = driver.find_element_by_class_name(row['Description_tag'])\n",
    "                if not content is None:\n",
    "                    description_elem = content.text\n",
    "            except KeyError:\n",
    "                continue\n",
    "            print('Source: ' + row['Site'])\n",
    "            if not title_elem is None:\n",
    "                print('Title: ' + unicodedata.normalize(normal_form, title_elem.text.strip()))\n",
    "            else:\n",
    "                print('Title: Not found')\n",
    "            if not company_elem is None:\n",
    "                print('Company: ' + unicodedata.normalize(normal_form, company_elem.text.strip()))\n",
    "            else:\n",
    "                print('Company: Not found')\n",
    "            if not location_elem is None:\n",
    "                print('Location: ' + unicodedata.normalize(normal_form, location_elem.text.strip()))\n",
    "            else:\n",
    "                print('Location: Not found')\n",
    "            if not description_elem is None:\n",
    "                print()\n",
    "                print('Description')\n",
    "                print('===========')\n",
    "                print(description_elem)\n",
    "                print()\n",
    "            else:\n",
    "                print('Description: Not found')\n",
    "            print(\"Link: \" + unicodedata.normalize(normal_form, URL))\n",
    "            print('=======================================================================================================================================================')\n",
    "            print()\n",
    "\n",
    "driver.quit()\n",
    "# Restore stdout\n",
    "sys.stdout = stdout_obj\n",
    "print(\"Finished successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to refactor the code into functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_by(row, upper_level_object, extraction_level):\n",
    "    if row['{}_attribute'.format(extraction_level)] == \"id\":\n",
    "        if len(upper_level_object.find_all(row['{}_name'.format(extraction_level)], id=row['{}_tag'.format(extraction_level)])) > 0:\n",
    "            return upper_level_object.find_all(row['{}_name'.format(extraction_level)], id=row['{}_tag'.format(extraction_level)])[row['{}_item'.format(extraction_level)]]\n",
    "    elif row['{}_attribute'.format(extraction_level)] == \"class\":\n",
    "        if len(upper_level_object.find_all(row['{}_name'.format(extraction_level)], class_=row['{}_tag'.format(extraction_level)])) > 0:\n",
    "            return upper_level_object.find_all(row['{}_name'.format(extraction_level)], class_=row['{}_tag'.format(extraction_level)])[row['{}_item'.format(extraction_level)]]\n",
    "    elif row['{}_attribute'.format(extraction_level)] == \"none\":\n",
    "        if len(upper_level_object.find_all(row['{}_name'.format(extraction_level)])) > 0:\n",
    "            return upper_level_object.find_all(row['{}_name'.format(extraction_level)])[int(row['{}_item'.format(extraction_level)])]\n",
    "    elif row['{}_attribute'.format(extraction_level)] == \"self\":\n",
    "        return upper_level_object\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_headless_driver():\n",
    "    # Headless option meaning not show browser window\n",
    "    options = Options()\n",
    "    options.headless = True\n",
    "    options.add_argument(\"--window-size=1920,1200\")\n",
    "\n",
    "    DRIVER_PATH = 'C:\\\\Users\\\\Adriel\\\\Downloads\\\\chromedriver_win32\\\\chromedriver'\n",
    "#     driver = webdriver.Chrome(executable_path=DRIVER_PATH)\n",
    "    return webdriver.Chrome(options=options, executable_path=DRIVER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_format_unicode(heading, bs_object, report_file, normal_form=\"NFKC\"):\n",
    "    if not bs_object is None:\n",
    "        report_file.write(heading + unicodedata.normalize(normal_form, bs_object.text.strip()) + '\\n')\n",
    "        return heading + unicodedata.normalize(normal_form, bs_object.text.strip()) + '\\n'\n",
    "    else:\n",
    "        report_file.write(heading + 'Not found\\n')\n",
    "        return heading + 'Not found\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\".FYwKg _3gJU3_6 _1yPon_6\"}\n  (Session info: headless chrome=87.0.4280.88)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-db6cf0398636>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m                     \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Description_tag'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Description_attribute'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m                     \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_class_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Description_tag'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontent\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m                     \u001b[0mdescription_elem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mfind_element_by_class_name\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    562\u001b[0m             \u001b[0melement\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_class_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'foo'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m         \"\"\"\n\u001b[1;32m--> 564\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCLASS_NAME\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    565\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfind_elements_by_class_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mfind_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    974\u001b[0m                 \u001b[0mby\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCSS_SELECTOR\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'[name=\"%s\"]'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 976\u001b[1;33m         return self.execute(Command.FIND_ELEMENT, {\n\u001b[0m\u001b[0;32m    977\u001b[0m             \u001b[1;34m'using'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    978\u001b[0m             'value': value})['value']\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[0;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alert'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\".FYwKg _3gJU3_6 _1yPon_6\"}\n  (Session info: headless chrome=87.0.4280.88)\n"
     ]
    }
   ],
   "source": [
    "current_datetime = datetime.now()\n",
    "driver = set_up_headless_driver()\n",
    "description_list = []\n",
    "count = 1\n",
    "\n",
    "with open(\"JobReport_\" + current_datetime.strftime(\"%Y%m%d_%H%M%S\") + \".txt\", \"w\", encoding = 'utf-8') as report_file:\n",
    "    report_file.write(\"Job Report on \" + current_datetime.strftime(\"%d/%m/%Y %H:%M:%S\") + '\\n')\n",
    "    report_file.write(\"=================================\\n\")\n",
    "    for index, row in job_ad_sites.iterrows():\n",
    "        page = requests.get(row['Listing_URL'])\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        results = find_all_by(soup, 'Result')\n",
    "        if row['Element_attribute'] == 'class':\n",
    "            job_elems = results.find_all(row['Element_name'], class_=row['Element_tag'])\n",
    "        elif row['Element_attribute'] == 'none':\n",
    "            job_elems = results.find_all(row['Element_name'])\n",
    "        \n",
    "        for job_elem in job_elems:\n",
    "            description_elem = None\n",
    "            content = None\n",
    "            title_elem = find_all_by(job_elem, 'Title')\n",
    "            company_elem = find_all_by(job_elem, 'Company')\n",
    "            location_elem = find_all_by(job_elem, 'Location')\n",
    "            URL = find_all_by(job_elem, 'URL')\n",
    "\n",
    "            if not any((title_elem, company_elem, location_elem, URL)):\n",
    "                continue\n",
    "\n",
    "            URL = URL['href']\n",
    "\n",
    "            if str.startswith(URL, '/'):\n",
    "                URL = row['Prefix_URL'] + URL\n",
    "\n",
    "            try:\n",
    "                page_content = driver.get(URL)\n",
    "                if row['Description_attribute'] == 'id':\n",
    "                    content = driver.find_element_by_id(row['Description_tag'])\n",
    "                elif row['Description_attribute'] == 'class':\n",
    "                    content = driver.find_element_by_class_name(row['Description_tag'])\n",
    "                elif row['Description_attribute'] == 'xpath':\n",
    "                    content = driver.find_element_by_xpath(row['Description_tag'])\n",
    "                if not content is None:\n",
    "                    description_elem = content.text\n",
    "            except KeyError:\n",
    "                continue\n",
    "            report_file.write(\"Job number: \" + str(count) + \"\\n\")\n",
    "            count += 1\n",
    "            report_file.write('Source: ' + row['Site'] + '\\n')\n",
    "            print_format_unicode('Title: ', title_elem, report_file)\n",
    "            print_format_unicode('Company: ', company_elem, report_file)\n",
    "            print_format_unicode('Location: ', location_elem, report_file)\n",
    "            if not description_elem is None:\n",
    "                report_file.write('\\n')\n",
    "                report_file.write('Description\\n')\n",
    "                report_file.write('===========\\n')\n",
    "                report_file.write(description_elem)\n",
    "                description_list.append(description_elem)\n",
    "                report_file.write('\\n')\n",
    "            else:\n",
    "                report_file.write('Description: Not found\\n')\n",
    "            report_file.write(\"Link: \" + unicodedata.normalize(\"NFKC\", URL) + '\\n')\n",
    "            report_file.write('=======================================================================================================================================================\\n')\n",
    "            report_file.write('\\n')\n",
    "\n",
    "driver.quit()\n",
    "print(\"Finished successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1716"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(description_elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Keyword extraction using TfIdf values\n",
    "### 2.1 Vocabulary list and count (Tf values)\n",
    "We now aim to extract the most used keyword considering all the job postings. This will hopefully give us some idea as to the most important skills associated with a certain role.\n",
    "\n",
    "Credits to tutorial on [FreeCodeCamp](https://www.freecodecamp.org/news/how-to-extract-keywords-from-text-with-tf-idf-and-pythons-scikit-learn-b2a0f3d7e667/).\n",
    "\n",
    "First of all, we will aim to create a vocabulary list of all job listings of a certain date, together with the number of occurences in each listing, using the ```CountVectorizer``` of ```scikit-learn```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def pre_process(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters that are of no use\n",
    "    # Mind the apostrophe ’ is not the same as the quote '\n",
    "    text = re.sub('(\\\\d|[^+#.’\\\\w\\'])+',' ', text)\n",
    "    # Remove full stop (recognised by space after it or end of string)\n",
    "    text = re.sub('\\\\.\\s', ' ', text)\n",
    "    text = re.sub('\\\\.$', ' ', text)\n",
    "    text = re.sub('’','\\'', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "\n",
    "def get_stop_words(stopwords_file_path):\n",
    "    # load stop words\n",
    "    with open(stopwords_file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        stopwords = f.readlines()\n",
    "        stop_set = set(m.strip() for m in stopwords)\n",
    "        return frozenset(stop_set)\n",
    "        \n",
    "# load stopwords\n",
    "stopwords = get_stop_words(\"Config\\stopwords.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(max_df=0.85, stop_words = stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_count_vector = count_vectorizer.fit_transform(description_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67, 3935)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['belong',\n",
       " 'twilio',\n",
       " 'who',\n",
       " 'rapidly',\n",
       " 'growing',\n",
       " 'leader',\n",
       " 'cloud',\n",
       " 'communications',\n",
       " 'software',\n",
       " 'market']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(count_vectorizer.vocabulary_.keys())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that company names can mix into our vocabulary list, let's try adding the company name into our stop word list, calling it ```stopwords_temp.txt```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished successfully\n"
     ]
    }
   ],
   "source": [
    "# Set up temp stop word list with company names from current search\n",
    "with open(\"stopwords_temp.txt\", \"w\") as ft:\n",
    "    f = open(\"stopwords.txt\",'r')\n",
    "    ft.write(f.read() + '\\n')\n",
    "    f.close()\n",
    "current_datetime = datetime.now()\n",
    "driver = set_up_headless_driver()\n",
    "description_list = []\n",
    "count = 1\n",
    "\n",
    "with open(\"JobReport_\" + current_datetime.strftime(\"%Y%m%d_%H%M%S\") + \".txt\", \"w\", encoding = 'utf-8') as report_file:\n",
    "    report_file.write(\"Job Report on \" + current_datetime.strftime(\"%d/%m/%Y %H:%M:%S\") + '\\n')\n",
    "    report_file.write(\"=================================\\n\")\n",
    "    for index, row in job_ad_sites.iterrows():\n",
    "        page = requests.get(row['Listing_URL'])\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        results = find_all_by(row, soup, 'Result')\n",
    "        if row['Element_attribute'] == 'class':\n",
    "            if row['Element_item'] == 'sub':\n",
    "                job_elems = results.find(row['Element_name'], class_=row['Element_tag']).find_all(row['Element_name'], recursive=False)\n",
    "            else:\n",
    "                job_elems = results.find_all(row['Element_name'], class_=row['Element_tag'])\n",
    "        elif row['Element_attribute'] == 'none':\n",
    "            job_elems = results.find_all(row['Element_name'])\n",
    "        \n",
    "        for job_elem in job_elems:\n",
    "            description_elem = None\n",
    "            content = None\n",
    "            title_elem = find_all_by(row, job_elem, 'Title')\n",
    "            company_elem = find_all_by(row, job_elem, 'Company')\n",
    "            location_elem = find_all_by(row, job_elem, 'Location')\n",
    "            URL = find_all_by(row, job_elem, 'URL')\n",
    "\n",
    "            if not any((title_elem, company_elem, location_elem, URL)):\n",
    "                continue\n",
    "\n",
    "            URL = URL['href']\n",
    "\n",
    "            if str.startswith(URL, '/'):\n",
    "                URL = row['Prefix_URL'] + URL\n",
    "\n",
    "            try:\n",
    "                page_content = driver.get(URL)\n",
    "                if row['Description_attribute'] == 'id':\n",
    "                    content = driver.find_element_by_id(row['Description_tag'])\n",
    "                elif row['Description_attribute'] == 'class':\n",
    "                    content = driver.find_element_by_class_name(row['Description_tag'])\n",
    "                elif row['Description_attribute'] == 'xpath':\n",
    "                    content = driver.find_element_by_xpath(row['Description_tag'])\n",
    "                if not content is None:\n",
    "                    description_elem = content.text\n",
    "            except KeyError:\n",
    "                continue\n",
    "            report_file.write(\"Job number: \" + str(count) + \"\\n\")\n",
    "            count += 1\n",
    "            report_file.write('Source: ' + row['Site'] + '\\n')\n",
    "            print_format_unicode('Title: ', title_elem, report_file)\n",
    "            print_format_unicode('Company: ', company_elem, report_file)\n",
    "            print_format_unicode('Location: ', location_elem, report_file)\n",
    "            if not description_elem is None:\n",
    "                report_file.write('\\n')\n",
    "                report_file.write('Description\\n')\n",
    "                report_file.write('===========\\n')\n",
    "                report_file.write(description_elem)\n",
    "                description_list.append(pre_process(description_elem))\n",
    "                report_file.write('\\n')\n",
    "            else:\n",
    "                report_file.write('Description: Not found\\n')\n",
    "            report_file.write(\"Link: \" + unicodedata.normalize(\"NFKC\", URL) + '\\n')\n",
    "            report_file.write('=======================================================================================================================================================\\n')\n",
    "            report_file.write('\\n')\n",
    "            \n",
    "            # Add company name into stopword list\n",
    "            if not company_elem is None:\n",
    "                with open(\"stopwords_temp.txt\", \"a\") as ft:\n",
    "                    ft.write(re.sub('(\\s+|\\n+)', '\\n', re.sub('\\s+\\W+\\s+', '\\n', company_elem.text.strip())) + '\\n')\n",
    "\n",
    "driver.quit()\n",
    "print(\"Finished successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['come', 'vis', 'viser', 'visest'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "# load stopwords\n",
    "stopwords_new = get_stop_words(\"stopwords.txt\")\n",
    "count_vectorizer_new = CountVectorizer(max_df=0.85, stop_words = stopwords_new)\n",
    "word_count_vector_new = count_vectorizer_new.fit_transform(description_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['belong',\n",
       " 'twilio',\n",
       " 'who',\n",
       " 'rapidly',\n",
       " 'growing',\n",
       " 'leader',\n",
       " 'cloud',\n",
       " 'communications',\n",
       " 'software',\n",
       " 'market']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(count_vectorizer_new.vocabulary_.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98, 4141)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_vector_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tf-Idf value calculation\n",
    "#### 2.2.1 Calculate Idf values from the library\n",
    "The Tf-Idf value will be the importance of each word in the text (in this case, the job descriptions).\n",
    "\n",
    "Term frequency of **a document** (Tf) is how frequent each vocabulary has appeared in the selected text, whereas the inverse document frequency (Idf) of the **library of documents** is how frequent each vocabulary has appeared in the library  (in this case, job descriptions across different sites).\n",
    "\n",
    "The Tf value should be directly proportional to the importance of each vocabulary, whereas the Idf value should be inversely proportional.\n",
    "\n",
    "The Tf-Idf value is the product of the two values.\n",
    "\n",
    "Now, we will use the ```TfIdf_Transformer``` in ```scikit-learn``` to obtain the Idf values of each word by fitting to the word count matrix we obtained in Section 2.1 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vector1 = tfidf_transformer.transform(count_vectorizer.transform([description_all]))\n",
    "sorted_items1 = sort_coo(tf_idf_vector1.tocoo())\n",
    "\n",
    "# Extract only top 10 results\n",
    "keywords1 = extract_top_n_from_vector(count_vectorizer.get_feature_names(), sorted_items1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'team': 0.186,\n",
       " 'web': 0.128,\n",
       " 'job': 0.123,\n",
       " 'technology': 0.113,\n",
       " 'data': 0.112,\n",
       " 'working': 0.107,\n",
       " 'end': 0.107,\n",
       " 'product': 0.102,\n",
       " 'company': 0.096,\n",
       " 'design': 0.095}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.95491028, 1.95343028, 2.23214368, ..., 4.87120101, 4.87120101,\n",
       "       4.87120101])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4049,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer.idf_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95, 4049)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Fitting to Tf value matrix\n",
    "After that, we can use the TfIdf transformer to fit into *each* job listing (which is the selected text) to obtain the TfIdf value of each word. This will tell us the important words in each job listing.\n",
    "\n",
    "We sort the words in the vector in **descending** order of TfIdf values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = count_vectorizer_new.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4184\n"
     ]
    }
   ],
   "source": [
    "print(len(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc = description_list[0]\n",
    "for description in description_list:\n",
    "    tf_idf_vector = tfidf_transformer.transform(count_vectorizer_new.transform([test_doc]))\n",
    "    # Sort the TfIdf vector by descending order of scores\n",
    "    sorted_items = sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "    # Extract only top 10 results\n",
    "    keywords = extract_top_n_from_vector(feature_names, sorted_items, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tf_idf_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2722, 0.3921116982223662), (1988, 0.25524316672679276), (3413, 0.25079581059176337), (3943, 0.22262785395658632), (1020, 0.17392571777476237), (1804, 0.14750135655592328), (3737, 0.1425711194708222), (770, 0.1425711194708222), (916, 0.14217620235678483), (2340, 0.13968491864324195), (596, 0.13729534882473024), (3801, 0.1307038994074554), (724, 0.1307038994074554), (3646, 0.12478893455795), (1712, 0.12284506537941935), (3960, 0.12228397277554406), (2954, 0.12228397277554406), (1559, 0.12228397277554406), (1900, 0.11575297028923275), (1706, 0.11278558170936821), (1337, 0.11041675271217724), (3509, 0.10590504503660662), (2889, 0.10590504503660662), (3633, 0.10491762417888391), (719, 0.10420395209193148), (3447, 0.10199682608026595), (2904, 0.10199682608026595), (2365, 0.10199682608026595), (3762, 0.0985495326488104), (182, 0.09331020729098502), (3820, 0.0901296060168991), (1534, 0.0901296060168991), (950, 0.0901296060168991), (736, 0.0901296060168991), (2915, 0.08778690276713834), (1274, 0.08778690276713834), (308, 0.08778690276713834), (191, 0.08778690276713834), (176, 0.08778690276713834), (1206, 0.08561789834132846), (3952, 0.08359860353058778), (3406, 0.08359860353058778), (2397, 0.0817096793849878), (1869, 0.0817096793849878), (1708, 0.0817096793849878), (1054, 0.0817096793849878), (302, 0.07993530789415634), (2988, 0.07826238595353227), (1641, 0.07667993754996225), (1041, 0.07667993754996225), (2205, 0.07375067827796164), (1749, 0.07375067827796164), (4015, 0.07238912274124386), (3272, 0.07238912274124386), (2676, 0.06984245932162098), (3268, 0.06864767441236512), (2654, 0.06864767441236512), (1470, 0.06864767441236512), (1421, 0.0674997560718602), (469, 0.06533075164605034), (174, 0.06533075164605034), (2523, 0.06331145683530963), (1023, 0.06331145683530963), (2902, 0.062351758860076115), (2090, 0.06142253268970967), (2500, 0.06052190267787704), (2120, 0.06052190267787704), (2642, 0.05964816119887821), (1548, 0.058799749159739005), (923, 0.057975239258254124), (480, 0.057975239258254124), (3794, 0.05717332153537544), (2907, 0.05717332153537544), (3212, 0.055632536008493355), (54, 0.05416882272083184), (434, 0.05346353158268351), (1978, 0.05210197604596574), (3089, 0.05080095448311428), (3671, 0.05017150730015959), (181, 0.05017150730015959), (2107, 0.04721260937658207), (1239, 0.04721260937658207), (2429, 0.045043604950772205), (4005, 0.04206461216479798), (3068, 0.04206461216479798), (1001, 0.04068160689027073), (4030, 0.03936101450360007), (1542, 0.03893373441974744), (3698, 0.0380974442138447)]\n"
     ]
    }
   ],
   "source": [
    "print(sort_coo(tf_idf_vector.tocoo()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_vector):\n",
    "    '''\n",
    "    Sorts the coordinate vector in descending order of values.\n",
    "    '''\n",
    "    tuples = zip(coo_vector.col, coo_vector.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_n_from_vector(feature_names, sorted_items, topn=10):\n",
    "    '''\n",
    "    Get the feature names and TfIdf score of top n items\n",
    "    '''\n",
    "    # Extract top n items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "    \n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # Word index and corresponding TfIdf score\n",
    "    for idx, score in sorted_items:\n",
    "        # Keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "        \n",
    "    # Create a dictionary of (feature, score)\n",
    "    results = {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]] = score_vals[idx]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Doc=====\n",
      "description because you belong at twilio the who what why and where twilio is a rapidly growing leader in the cloud communications software market and we are looking for top tier proven sales account executives aes who are looking to grow their career in the fast growing cloud communications platform market this role reports to senior manager growth mid market based in singapore who as strategic mid market account executive you will be responsible for selling to mid market customers developing a relationship as a trusted advisor and deeply understanding their unique challenges and goals you will contribute to our business growth in a fast paced collaborative and fun atmosphere the right candidate will have a proven consultative sales process to discover and close new logos our aes develop an understanding of prospects' businesses organize and conduct sales presentations at prospective and current customers' offices and represent twilio in a consistent effective and professional manner to best develop and win new clients we are looking for an experienced ae with a proven track record and also yrs of quota carrying and account management experience in selling infrastructure software or platform solutions to company sizes ranging from mid market to small enterprise companies consistent and demonstrable over achievement in past sales roles credible trusted partner with customers and their executive team an exceptional prospecting skill set someone who is comfortable generating the bulk of their own pipeline experience with cloud computing business models and selling to a technical audience from a position of credibility and trust proficiency in salesforce.com for tracking sales activity pipeline and revenue metrics experience with a value based sales process and the capability to build roi models as part of your sales methodology an ability to balance competing priorities and manage multiple projects deals at the same time ethical hands on passionate persistent takes initiative creative personable self starter productive competitive spirit resourceful coachable drive to overachieve please note that business mandarin proficiency is required for this position what strategic mid market account executives in apac are responsible for developing the next wave of twilio's new customers with companies that have employees aes build new relationships with senior line of business owners and executive stakeholders ctos cios cpo vp of product to develop sales by understanding and uncovering new opportunities where twilio can help solve company's pains and challenges through you will be an owner responsible for new customer acquisition and driving new revenue for a specific territory while maintaining the highest levels of customer satisfaction draw the owl with your world class interpersonal and communication skills you will make complex contractual technical and financial details sound simple wear the customer's shoes.create pricing proposals negotiate terms and manage the contract process whilst building trust and mutual respect with customers and peers ruthlessly prioritize balance competing priorities and manage multiple project deals at the same time travel will be required upon need why twilio is a company that is empowering the world's developers with modern communication tools in order to build better applications to maximize customer engagement the global sales team plays an integral role in building out our customer base and bringing twilio to developers lines of business non profits and enterprises to make an impact on their services where this position will be located in our beautiful wework co working space in causeway bay in hong kong you will enjoy our incredible perks snacks weekly team lunch daily community events vibrant co working space filled with multinational companies startups entrepreneurs freelancers and more what you will also get to experience is a company that believes in small teams for maximum impact that strives to balance work and home life that understands that this is a marathon not a sprint that continuously and purposefully builds an inclusive culture where everyone is able to do and be the best version of themselves we seek people who naturally demonstrate ourvalues who are challenged by problems empower others to thrive people who can draw the owl and not be beholden to one playbook about us millions of developers around the world have used twilio to unlock the magic of communications to improve any human experience twilio has democratized communications channels like voice text chat video and email by virtualizing the world's communications infrastructure through apis that are simple enough for any developer to use yet robust enough to power the world's most demanding applications by making communications a part of every software developer's toolkit twilio is enabling innovators across every industry from emerging leaders to the world's largest organizations to reinvent how companies engage with their customers \n",
      "\n",
      "===Keywords===\n",
      "twilio 0.39\n",
      "sales 0.266\n",
      "communications 0.211\n",
      "market 0.202\n",
      "mid 0.176\n",
      "who 0.142\n",
      "aes 0.136\n",
      "account 0.135\n",
      "customers 0.132\n",
      "selling 0.125\n"
     ]
    }
   ],
   "source": [
    "# Sort the TfIdf vector by descending order of scores\n",
    "sorted_items = sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "# Extract only top 10 results\n",
    "keywords = extract_top_n_from_vector(feature_names, sorted_items, 10)\n",
    "\n",
    "# Print results\n",
    "print(\"\\n=====Doc=====\")\n",
    "print(test_doc)\n",
    "print(\"\\n===Keywords===\")\n",
    "for k in keywords:\n",
    "    print(k, keywords[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'portal': 0.392, 'job': 0.255, 'specified': 0.251, 'web': 0.223, 'develop': 0.174, 'implement': 0.148, 'traffic': 0.143, 'construction': 0.143, 'database': 0.142, 'mobile': 0.14}\n"
     ]
    }
   ],
   "source": [
    "# Sort the TfIdf vector by descending order of scores\n",
    "sorted_items = sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "# Extract only top 10 results\n",
    "keywords = extract_top_n_from_vector(count_vectorizer.get_feature_names(), sorted_items, 10)\n",
    "\n",
    "# Print results\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine with the scraping code to produce a simplified report with 10 keywords of each job instead of whole description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['come', 'vis', 'viser', 'visest'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished successfully\n"
     ]
    }
   ],
   "source": [
    "current_datetime = datetime.now()\n",
    "driver = set_up_headless_driver()\n",
    "stopwords = get_stop_words(\"Config\\stopwords.txt\")\n",
    "count_vectorizer = CountVectorizer(max_df=0.85, stop_words = stopwords)\n",
    "tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "site_list = []\n",
    "title_list = []\n",
    "company_list = []\n",
    "location_list = []\n",
    "description_list = []\n",
    "link_list = []\n",
    "count = 1\n",
    "\n",
    "with open(\"JobReport_\" + current_datetime.strftime(\"%Y%m%d_%H%M%S\") + \".txt\", \"w\", encoding = 'utf-8') as report_file:\n",
    "    # TODO Change job role to variable\n",
    "    report_file.write(\"Software Developer Job Report on \" + current_datetime.strftime(\"%d/%m/%Y %H:%M:%S\") + '\\n')\n",
    "    report_file.write(\"=================================\\n\")\n",
    "    \n",
    "    for index, row in job_ad_sites.iterrows():\n",
    "        page = requests.get(row['Listing_URL'])\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        results = find_all_by(row, soup, 'Result')\n",
    "        if row['Element_attribute'] == 'class':\n",
    "            if row['Element_item'] == 'sub':\n",
    "                job_elems = results.find(row['Element_name'], class_=row['Element_tag']).find_all(row['Element_name'], recursive=False)\n",
    "            else:\n",
    "                job_elems = results.find_all(row['Element_name'], class_=row['Element_tag'])\n",
    "        elif row['Element_attribute'] == 'none':\n",
    "            job_elems = results.find_all(row['Element_name'])\n",
    "        \n",
    "        for job_elem in job_elems:\n",
    "            description_elem = None\n",
    "            content = None\n",
    "            title_elem = find_all_by(row, job_elem, 'Title')\n",
    "            company_elem = find_all_by(row, job_elem, 'Company')\n",
    "            location_elem = find_all_by(row, job_elem, 'Location')\n",
    "            URL = find_all_by(row, job_elem, 'URL')\n",
    "\n",
    "            if not any((title_elem, company_elem, location_elem, URL)):\n",
    "                continue\n",
    "\n",
    "            URL = URL['href']\n",
    "\n",
    "            if str.startswith(URL, '/'):\n",
    "                URL = row['Prefix_URL'] + URL\n",
    "\n",
    "            try:\n",
    "                page_content = driver.get(URL)\n",
    "                if row['Description_attribute'] == 'id':\n",
    "                    content = driver.find_element_by_id(row['Description_tag'])\n",
    "                elif row['Description_attribute'] == 'class':\n",
    "                    content = driver.find_element_by_class_name(row['Description_tag'])\n",
    "                elif row['Description_attribute'] == 'xpath':\n",
    "                    content = driver.find_element_by_xpath(row['Description_tag'])\n",
    "                if not content is None:\n",
    "                    description_elem = content.text\n",
    "            except KeyError:\n",
    "                continue\n",
    "            report_file.write(\"Job number: \" + str(count) + \"\\n\")\n",
    "            count += 1\n",
    "            \n",
    "            report_file.write('Source: ' + row['Site'] + '\\n')\n",
    "            site_list.append('Source: ' + row['Site'] + '\\n')\n",
    "            title_list.append(print_format_unicode('Title: ', title_elem, report_file))\n",
    "            company_list.append(print_format_unicode('Company: ', company_elem, report_file))\n",
    "            location_list.append(print_format_unicode('Location: ', location_elem, report_file))\n",
    "            \n",
    "            if not description_elem is None:\n",
    "                report_file.write('\\n')\n",
    "                report_file.write('Description\\n')\n",
    "                report_file.write('===========\\n')\n",
    "                report_file.write(description_elem)\n",
    "                description_list.append(pre_process(description_elem))\n",
    "                report_file.write('\\n')\n",
    "            else:\n",
    "                report_file.write('Description: Not found\\n')\n",
    "            report_file.write(\"Link: \" + unicodedata.normalize(\"NFKC\", URL) + '\\n')\n",
    "            link_list.append(\"Link: \" + unicodedata.normalize(\"NFKC\", URL) + '\\n')\n",
    "            report_file.write('=======================================================================================================================================================\\n')\n",
    "            report_file.write('\\n')\n",
    "\n",
    "# Extract keywords of this job description\n",
    "word_count_vector = count_vectorizer.fit_transform(description_list)\n",
    "tfidf_transformer.fit(word_count_vector)\n",
    "\n",
    "with open(\"JobKeyword_\" + current_datetime.strftime(\"%Y%m%d_%H%M%S\") + \".txt\", \"w\", encoding = 'utf-8') as keyword_file:\n",
    "    # TODO Change job role to variable\n",
    "    keyword_file.write(\"Software Developer Job Report (Extract) on \" + current_datetime.strftime(\"%d/%m/%Y %H:%M:%S\") + '\\n')\n",
    "    keyword_file.write(\"=================================\\n\")\n",
    "    \n",
    "    for i in range(len(site_list)):\n",
    "        keyword_file.write(\"Job number: \" + str(i+1) + \"\\n\")\n",
    "        keyword_file.write(site_list[i])\n",
    "        keyword_file.write(title_list[i])\n",
    "        keyword_file.write(company_list[i])\n",
    "        keyword_file.write(location_list[i])\n",
    "        \n",
    "        tf_idf_vector = tfidf_transformer.transform(count_vectorizer.transform([description_list[i]]))\n",
    "        # Sort the TfIdf vector by descending order of scores\n",
    "        sorted_items = sort_coo(tf_idf_vector.tocoo())\n",
    "        keywords = extract_top_n_from_vector(count_vectorizer.get_feature_names(), sorted_items, 10)\n",
    "        \n",
    "        keyword_string = \"\"\n",
    "        for k in keywords:\n",
    "            keyword_string += k.title() + \" \"\n",
    "        keyword_file.write(keyword_string + \"\\n\")\n",
    "        keyword_file.write(link_list[i])\n",
    "        keyword_file.write('=======================================================================================================================================================\\n')\n",
    "        keyword_file.write('\\n')\n",
    "        \n",
    "driver.quit()\n",
    "\n",
    "print(\"Finished successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
